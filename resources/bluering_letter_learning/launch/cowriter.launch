<launch>

    <!-- For interactive_learning -->
    <arg name="use_robot_in_interaction" default="true" />
    <arg name="nao_ip" />
    <arg name="nao_handedness" default="right" />

    <!-- For interactive_learning and tablet_input_interpreter -->
    <arg name="shape_feedback_topic" default="shape_feedback" />
    <arg name="gesture_info_topic" default="gesture_info" />
    <arg name="user_drawn_shapes_topic" default="user_drawn_shapes" />

    <!-- topic to receive the word to write -->
    <arg name="words_to_write_topic" default="words_to_write" />


    <!-- For word_card_detector -->
    <arg name="use_external_camera" default="true" /> <!-- if false, camera on robot will be used -->
    <arg name="camera_device" default="/dev/video0" /> <!-- only used if using external camera -->


    <!-- ======================== BEGIN learner params ======================== -->
    <!-- Where the datasets for generating letter models for the learning algorithm are stored -->
    <arg name="letter_model_dataset_directory" />
    <!-- Where to store log for this session -->
    <arg name="shape_log" />
    <!-- ======================== END   learner params ======================== -->



    <!-- ======================== BEGIN trajectory_following params ======================== -->
    <arg name="writing_surface_frame_id" default="writing_surface" />

    <!-- Method for positioning the writing surface. 'interactive_marker' for manual positioning in RViz, 'fiducial_marker_detection' for detecting a fiducial marker at the bottom left of the surface (tested with chilitags) -->
    <arg name="writing_surface_positioning_method" default="interactive_marker" />

    <!-- Name of frame of fiducial marker if used as positioning mode -->
    <arg name="fiducial_marker_frame_id" default="tag_1" />

    <!-- If chilitags is used as the fiducial marker detection scheme, the coordinate system of the detected tag has x up, y to the right. In future versions this may change, but for now it's necessary to rotate the frame, so the default is 'true.' If 'false' and no other processing is necessary, probably the frame can just be re-mapped and this node is unnecessary. -->
    <arg name="rotate_detected_tag_frame" default="true" />

    <!-- Size of rectangular marker to be displayed (default values for the galaxy note 10.1 in landscape orientation, use (0.21,0.297) for A4 paper in portrait orientation) -->
    <arg name="writing_surface_width_m" default="0.217" />
    <arg name="writing_surface_height_m" default="0.136" />
    <!-- ======================== END   trajectory_following params ======================== -->





    <!-- ======================== BEGIN diagram_manager params ======================== -->
    <arg name="use_external_microphone" default="true" /> <!-- if false, microphone on robot will be used -->

    <!-- params for node audio_capture -->
    <arg name="dst" default="appsink"/>
    <arg name="device" default=""/>
    <arg name="format" default="wave"/>
    <arg name="bitrate" default="128"/>
    <arg name="channels" default="1"/>
    <arg name="depth" default="16"/>
    <arg name="sample_rate" default="16000"/>
    <arg name="sample_format" default="S16LE"/>
    <arg name="ns" default="audio"/>
    <arg name="audio_topic" default="audio"/>


    <!-- params for node diagram_manager -->
    <arg name="min_silent_chunk_to_split" default="100"/>
    <arg name="language_code" default="en-US"/>
    <!-- Do we want to log audio data to file -->
    <arg name="log_audio_to_file" default="false" />
    <!-- If yes, then to which path -->
    <arg name="audio_outfile" default="" />
    <!-- Minimum buffer size before transcribing -->
    <arg name="audio_buflen" default="10240" />
    <!-- Threshold to detect an audio chunk is silence -->
    <arg name="silent_threshold" default="700" />
    <!-- when robot wants to perform any action (either speak or respond to a voice command), diagram_manager send to this topic, interactive_learning subscribe to control the robot to do the action -->
    <arg name="action_todo_topic" default="action_todo" />
    <!-- diagram_manager listen to this topic to know if it wants to capture audio input -->
    <arg name="listening_signal_topic" default="listening_signal" />

    <!-- configs for openai -->
    <arg name="openai_apikey" default="" />
    <arg name="openai_model" default="text-davinci-002" />
    <arg name="openai_temp" default="0.5" />
    <arg name="openai_max_tokens" default="1024" />
    <arg name="openai_timeout" default="20" />
    <!-- ======================== END   diagram_manager params ======================== -->





    <!-- Start the wode_card_detector to detect word from card -->
    <include file="$(find bluering_letter_learning)/launch/word_card_detector.launch">
        <!-- all vars that this launch file requires must be set -->
        <arg name="use_external_camera" value="$(arg use_external_camera)" /> <!-- if false, camera on robot will be used -->
        <arg name="camera_device" value="$(arg camera_device)" /> <!-- only used if using external camera -->
        <arg name="detected_words_topic" value="$(arg words_to_write_topic)" />
    </include>


    <!-- Start the display_manager_server, which will be accessed by the gesture_manager node and
    the relevant shape_learning package's node -->
    <include file="$(find bluering_letter_learning)/launch/display_manager_server.launch">
        <!-- all vars that this launch file requires must be set -->
    </include>


    <!-- Start the tablet_input_interpreter, which listens to gestures signals, and maps gestures to the relevant shape -->
    <include file="$(find bluering_letter_learning)/launch/tablet_input_interpreter.launch">
        <!-- all vars that this launch file requires must be set -->
        <arg name="shape_feedback_topic" value="$(arg shape_feedback_topic)" />
        <arg name="gesture_info_topic" value="$(arg gesture_info_topic)" />
        <arg name="touch_info_topic" default="touch_info" />
        <arg name="user_drawn_shapes_topic" value="$(arg user_drawn_shapes_topic)" />
    </include>



    <!-- Start interactive_learning -->
    <include file="$(find bluering_letter_learning)/launch/interactive_learning.launch">
        <!-- all vars that this launch file requires must be passed -->
        <arg name="use_robot_in_interaction" default="true" />
        <arg name="nao_ip" value="$(arg nao_ip)" />

        <arg name="nao_writing" value="$(arg use_robot_in_interaction)" />
        <arg name="nao_standing" default="true" />
        <arg name="nao_speaking" default="true" />
        <arg name="nao_handedness" value="$(arg nao_handedness)" />
        <arg name="person_side" value="$(arg nao_handedness)" />

        <arg name="language" default="english" />

        <!-- learner model -->
        <arg name="letter_model_dataset_directory" value="$(arg letter_model_dataset_directory)" />
        <arg name="shape_log" value="$(arg shape_log)" />

        <!-- I/O topics -->
        <arg name="words_to_write_topic" value="$(arg words_to_write_topic)" />
        <arg name="shape_feedback_topic" value="$(arg shape_feedback_topic)" />
        <arg name="gesture_info_topic" value="$(arg gesture_info_topic)" />
        <arg name="shape_writing_finished_topic" default="shape_finished" />
        <arg name="new_teacher_topic" default="new_child" />
        <arg name="stop_request_topic" default="stop_learning" />
        <arg name="test_request_topic" default="test_learning" />

        <arg name="clear_writing_surface_topic" default="clear_screen" />
        <arg name="camera_publishing_status_topic" default="camera_publishing_status" />

        <!-- topics to receive command or response from diagram_manager -->
        <arg name="action_todo_topic" value="$(arg action_todo_topic)" />

        <!-- topic to send listening signal (eg. when the robot speaks, send a signal to tell the diagram_manager stop listening) -->
        <arg name="listening_signal_topic" value="$(arg listening_signal_topic)" />



        <!-- trajectory_following supports -->
        <arg name="writing_surface_frame_id" value="$(arg writing_surface_frame_id)" />
        <arg name="writing_surface_positioning_method" value="$(arg writing_surface_positioning_method)" />
        <arg name="fiducial_marker_frame_id" value="$(arg fiducial_marker_frame_id)" />
        <arg name="rotate_detected_tag_frame" value="$(arg rotate_detected_tag_frame)" />
        <arg name="writing_surface_width_m" value="$(arg writing_surface_width_m)" />
        <arg name="writing_surface_height_m" value="$(arg writing_surface_height_m)" />

    </include>




    <!-- Start the diagram manager, which will capture audio and synthesize speech -->
    <include file="$(find bluering_letter_learning)/launch/diagram_manager.launch">
        <!-- all vars that this launch file requires must be set -->
        <arg name="dst" value="$(arg dst)"/>
        <arg name="device" value="$(arg device)"/>
        <arg name="format" value="$(arg format)"/>
        <arg name="bitrate" value="$(arg bitrate)"/>
        <arg name="channels" value="$(arg channels)"/>
        <arg name="depth" value="$(arg depth)"/>
        <arg name="sample_rate" value="$(arg sample_rate)"/>
        <arg name="sample_format" value="$(arg sample_format)"/>
        <arg name="ns" value="$(arg ns)"/>
        <arg name="audio_topic" value="$(arg audio_topic)"/>

        <arg name="min_silent_chunk_to_split" value="$(arg min_silent_chunk_to_split)"/>
        <arg name="language" value="$(arg language_code)"/>
        <arg name="log_audio_to_file" value="$(arg log_audio_to_file)" />
        <arg name="audio_outfile" value="$(arg audio_outfile)" />
        <arg name="audio_buflen" default="$(arg audio_buflen)" />
        <arg name="silent_threshold" value="$(arg silent_threshold)" />
        <!-- topics to send command or response from diagram_manager -->
        <arg name="action_todo_topic" value="$(arg action_todo_topic)" />
        <!-- topic to receive listening signal (eg. when the robot speaks, send a signal to tell the diagram_manager stop listening) -->
        <arg name="listening_signal_topic" value="$(arg listening_signal_topic)" />
        <arg name="words_to_write_topic" value="$(arg words_to_write_topic)" />

        <arg name="openai_apikey" value="$(arg openai_apikey)" />
        <arg name="openai_model" value="$(arg openai_model)" />
        <arg name="openai_temp" value="$(arg openai_temp)" />
        <arg name="openai_max_tokens" value="$(arg openai_max_tokens)" />
        <arg name="openai_timeout" value="$(arg openai_timeout)" />
    </include>



</launch>